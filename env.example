# ─────────────────────────────────────────────────────────────────────────────
# T&C Analyzer — Environment Variables
# Copy this file to .env and fill in your values
# cp .env.example .env
# ─────────────────────────────────────────────────────────────────────────────

# Flask secret key — CHANGE THIS in production!
# Generate one with: python -c "import secrets; print(secrets.token_hex(32))"
SECRET_KEY=tc-analyzer-secret-change-in-prod

# Flask environment: production | development
FLASK_ENV=production

# Port the app listens on inside the container
PORT=5050

# ── Ollama / LLM settings ──────────────────────────────────────────────────
# URL of the Ollama server (use container name when running via Docker Compose)
OLLAMA_BASE_URL=http://ollama:11434

# Model to use — must be pulled first: docker compose exec ollama ollama pull llama3.2
# Good small models: llama3.2 (3B), phi3, mistral, gemma2:2b
OLLAMA_MODEL=llama3.2

# Timeout in seconds for LLM responses (increase for slower hardware)
OLLAMA_TIMEOUT=120

# Set to false to disable LLM features entirely
OLLAMA_ENABLED=true
